{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 基本运算示例\n",
    "\n",
    "Torch的tensor和Numpy的Array，Pandas的Series和Dataframe类似，是了解应用这些代码库的基础，这里首先简单快速地了解Tensor及如何使用GPU，然后日常积累一些常用张量运算函数。\n",
    "\n",
    "主要参考：\n",
    "\n",
    "- [TENSORS](https://pytorch.org/tutorials/beginner/former_torchies/tensor_tutorial.html)\n",
    "- [Speed Up your Algorithms Part 1 — PyTorch](https://towardsdatascience.com/speed-up-your-algorithms-part-1-pytorch-56d8a4ae7051)\n",
    "- [PyTorch 101, Part 1: Understanding Graphs, Automatic Differentiation and Autograd](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 快速了解Tensor\n",
    "\n",
    "pytorch作为NumPy的替代品，可以利用GPU的性能进行计算；可作为一个高灵活性、速度快的深度学习平台。\n",
    "\n",
    "Tensor（张量）类似于NumPy的ndarray，但还可以在GPU上使用来加速计算。因此经常看到把numpy的数组包装为tensor再运算。tensor的操作和numpy中的数组操作类似，不再赘述，详见官网。下面列举一些简单例子。首先pytorch的导入是import torch，因为torch一直都是那个torch，一开始是别的语言写的，现在在python下，所以就叫pytorch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor是pytorch的基本数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(5)\n",
    "# 如果想要从tensor中获取到长度的int数据\n",
    "type(list(x.shape)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9184e-39, 8.7245e-39, 9.2755e-39],\n",
       "        [8.9082e-39, 9.9184e-39, 8.4490e-39],\n",
       "        [9.6429e-39, 1.0653e-38, 1.0469e-38],\n",
       "        [4.2246e-39, 1.0378e-38, 9.6429e-39],\n",
       "        [9.2755e-39, 9.7346e-39, 1.0745e-38]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建一个 5x3 的矩阵, 未初始化的:\n",
    "x = torch.Tensor(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch中的一些基本运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4398, 0.1398, 0.7243],\n",
       "        [0.2603, 0.5517, 0.9940],\n",
       "        [0.8248, 0.4888, 0.6620],\n",
       "        [0.1417, 0.2469, 0.4292],\n",
       "        [0.4014, 0.0369, 0.4385]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建一个随机初始化的矩阵:\n",
    "x = torch.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatenate的操作使用torch.cat可以完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 8.5451e-07])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.Tensor(1)\n",
    "x2 = torch.Tensor(1)\n",
    "torch.cat((x1,x2),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到torch中的size也是torch中的类，包装了python的list，自然地，加减运算的对象也都是torch的tensor了。运算可以使用运算符，也可以使用函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8916, 0.9551, 1.1712],\n",
       "        [0.5677, 0.5823, 1.0931],\n",
       "        [1.4513, 1.4779, 1.5074],\n",
       "        [0.8993, 0.2620, 0.7302],\n",
       "        [1.3191, 0.1133, 1.2089]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加法\n",
    "y = torch.rand(5, 3)\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8916, 0.9551, 1.1712],\n",
       "        [0.5677, 0.5823, 1.0931],\n",
       "        [1.4513, 1.4779, 1.5074],\n",
       "        [0.8993, 0.2620, 0.7302],\n",
       "        [1.3191, 0.1133, 1.2089]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8916, 0.9551, 1.1712],\n",
       "        [0.5677, 0.5823, 1.0931],\n",
       "        [1.4513, 1.4779, 1.5074],\n",
       "        [0.8993, 0.2620, 0.7302],\n",
       "        [1.3191, 0.1133, 1.2089]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out = result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a.add_(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy和tensor之间有很多类似的地方，比如索引，形状改变等："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1398, 0.5517, 0.4888, 0.2469, 0.0369])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以用类似Numpy的索引来处理所有的张量！\n",
    "x[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "tensor([[-0.1869, -1.4803,  0.7319, -0.8843],\n",
      "        [ 0.8513,  0.9019, -0.6537, -0.3310],\n",
      "        [-0.4791, -0.5095, -0.0829,  0.9046],\n",
      "        [-0.0829,  0.5256,  0.1591,  0.3896]])\n",
      "tensor([-0.1869, -1.4803,  0.7319, -0.8843,  0.8513,  0.9019, -0.6537, -0.3310,\n",
      "        -0.4791, -0.5095, -0.0829,  0.9046, -0.0829,  0.5256,  0.1591,  0.3896])\n",
      "tensor([[-0.1869, -1.4803,  0.7319, -0.8843,  0.8513,  0.9019, -0.6537, -0.3310],\n",
      "        [-0.4791, -0.5095, -0.0829,  0.9046, -0.0829,  0.5256,  0.1591,  0.3896]])\n"
     ]
    }
   ],
   "source": [
    "# 改变大小: 如果你想要去改变tensor的大小, 可以使用 torch.view:\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # -1就是让pytorch自己根据其他的维度去判断这里该是几维\n",
    "print(x.size(), y.size(), z.size())\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于和numpy的紧密联系，因此pytorch的张量和numpy数组可以很方便的转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要注意numpy的array和torch的tensor转换后，数据是绑定的，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy的array： [1. 1. 1. 1. 1.]\n",
      "array转为torch的tensor： tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 看改变 np 数组之后 Torch Tensor 是如何自动改变的\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(\"numpy的array：\",a)\n",
    "print(\"array转为torch的tensor：\",b)\n",
    "np.add(a, 1, out = a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11445\\miniconda3\\envs\\hydrus\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# 可以使用 .cuda 方法将 Tensors 在GPU上运行.\n",
    "# 只要在  CUDA 是可用的情况下, 我们可以运行这段代码\n",
    "if torch.cuda.is_available():\n",
    "    b = b.cuda()\n",
    "    print(b + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU上的所有张量(CharTensor除外)都支持与Numpy的相互转换。\n",
    "\n",
    "张量要在GPU上计算，需要主动从CPU移动到GPU上。张量可以使用.to方法移动到任何设备（device）上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9909])\n",
      "0.9909061193466187\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())\n",
    "# 当GPU可用时,我们可以运行以下代码\n",
    "# 我们将使用`torch.device`来将tensor移入和移出GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # 直接在GPU上创建tensor\n",
    "    x = x.to(device)                       # 或者使用`.to(\"cuda\")`方法\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # `.to`也能在移动时改变dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU or CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查 cuda 设备是否可用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11445\\miniconda3\\envs\\hydrus\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果有GPU的话 可以执行下面几个注释掉的代码\n",
    "# torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the current GPU memory usage by tensors in bytes for a given device\n",
    "# torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the current GPU memory managed by the caching allocator in bytes for a given device\n",
    "# torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何使用GPU？\n",
    "\n",
    "如果像存储数据到CPU，那么简单定义即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.DoubleTensor([1., 2.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时候变量在CPU上，且后续的运算都会在CPU上，为了将数据转移到GPU上，需要使用 .cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.FloatTensor([1., 2.]).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.cuda.FloatTensor([1., 2.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时候数据就在GPU上。模型同样可以转移到GPU上，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "model = nn.Sequential(\n",
    "         nn.Linear(20, 20),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(20, 4),\n",
    "         nn.Softmax()\n",
    ")\n",
    "# model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查在不在GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果有多个GPU，可以使用Data Parallelism，拆分数据，将 Data Generator 拆分到更小的mini batches，然后送到多个GPUs\n",
    "\n",
    "PyTorch中，并行通过torch.nn.DataParallel实现。一些关键函数：：\n",
    "\n",
    "- Replicate：Module在多个设备上复制。\n",
    "- Scatter：input在这些设备的第一维中分布。\n",
    "- Gather：从这些设备收集input和连接第一维。\n",
    "- parallel_apply：将从Scatter获得的一组输入分布式应用于从Replicate获得的相应的分布式Modules。\n",
    "\n",
    "```Python\n",
    "# Replicate module to devices in device_ids\n",
    "replicas = nn.parallel.replicate(module, device_ids)\n",
    "# Distribute input to devices in device_ids\n",
    "inputs = nn.parallel.scatter(input, device_ids)\n",
    "# Apply the models to corresponding inputs\n",
    "outputs = nn.parallel.parallel_apply(replicas, inputs)\n",
    "# Gather result from all devices to output_device\n",
    "result = nn.parallel.gather(outputs, output_device)\n",
    "```\n",
    "\n",
    "或者一种更简单的方式：\n",
    "\n",
    "```Python\n",
    "model = nn.DataParallel(model, device_ids=device_ids)\n",
    "result = model(input)\n",
    "```\n",
    "\n",
    "更多信息可以关注：\n",
    "\n",
    "- [Multi-GPU Framework Comparisons](https://medium.com/@iliakarmanov/multi-gpu-rosetta-stone-d4fa96162986)\n",
    "- [ilkarman/DeepLearningFrameworks](https://github.com/ilkarman/DeepLearningFrameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是Tensor的常见计算，日常积累。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本算术运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor是一种数据结构，是 PyTorch 的基本构建块。Tensors 非常像 numpy 数组，与 numpy 不同的是，张量旨在利用 GPU 的并行计算能力。许多 Tensor 语法类似于 numpy 数组的语法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化\n",
    "\n",
    "直接调用Tensor即可完成初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = torch.Tensor(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和numpy不同的是，为了完成梯度下降等优化计算，Tensor还有梯度相关的属性（后续文本会有更多介绍），初始化时候requires_grad默认是False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_one = torch.ones(4)\n",
    "t_one.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requires_grad具有传染性。这意味着当通过对其他Tensors运算创建一个Tensor，且其中至少有一个用于创建的张量requires_grad被设置为True时，运算结果Tensor的requires_grad也将被设置为True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((3,3), requires_grad = True)\n",
    "w1 = torch.randn((3,3))\n",
    "b = w1*a \n",
    "b.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想要计算后的张量不被前序变量传染，可以使用detach()（非inplace运算），先detach()再计算即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_detach = w1*(a.detach())\n",
    "b_detach == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_detach.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detach()不是inplace运算，所以a还是有requires_grad的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 均值计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.44499848 0.06677202 0.48286518 0.79960818]\n",
      " [0.55011906 0.84638496 0.9129834  0.72624932]\n",
      " [0.68279485 0.72864658 0.56643253 0.85884824]\n",
      " [0.01584725 0.83010497 0.17834428 0.8620229 ]\n",
      " [0.8497338  0.66295321 0.10840663 0.52652553]\n",
      " [0.22564929 0.36593341 0.9118287  0.71547575]] (6, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.46152379, 0.58346586, 0.52681012, 0.74812165]),\n",
       " array([6., 6., 6., 6.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(6,4)\n",
    "print(X,X.shape)\n",
    "avg_np, _ = np.average(X, axis=0, returned=True)\n",
    "avg_np, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4615, 0.5835, 0.5268, 0.7481], dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_th = torch.tensor(X)\n",
    "avg_th = torch.mean(X_th, dim=0)\n",
    "avg_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (X == X_th.numpy()).all()\n",
    "# assert (avg_np == avg_th.numpy()).all()   # this fails alread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 求和计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求和运算和平均运算类似："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta=torch.ones(5)\n",
    "tc=torch.sum(ta)\n",
    "tc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch中的广播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试试torch的广播功能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
    "t2=torch.sqrt(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8264, 0.8723, 0.8938],\n",
       "        [0.9070, 0.9162, 0.9231]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1/((t2+0.1)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### element-wise运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta=torch.ones(5)\n",
    "tb=torch.zeros(5)\n",
    "ta-tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.FloatTensor([2]).repeat(1,5)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z/ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td=ta/z\n",
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te=td**2\n",
    "te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看二维的情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [4., 5., 6.],\n",
       "        [4., 5., 6.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
    "t2=t1.repeat(1,4).view(-1, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.,  8.,  9.],\n",
       "        [ 7.,  8.,  9.],\n",
       "        [ 7.,  8.,  9.],\n",
       "        [ 7.,  8.,  9.],\n",
       "        [10., 11., 12.],\n",
       "        [10., 11., 12.],\n",
       "        [10., 11., 12.],\n",
       "        [10., 11., 12.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.FloatTensor([[7,8,9],[10,11,12]])\n",
    "t4=t3.repeat(1,4).view(-1, 3)\n",
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[36., 36., 36.],\n",
       "        [36., 36., 36.],\n",
       "        [36., 36., 36.],\n",
       "        [36., 36., 36.],\n",
       "        [36., 36., 36.],\n",
       "        [36., 36., 36.],\n",
       "        [36., 36., 36.],\n",
       "        [36., 36., 36.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SST = (t4 - t2) ** 2\n",
    "SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([288., 288., 288.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SST=torch.sum(SST,dim=0)\n",
    "SST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比较大小\n",
    "\n",
    "两个张量，element-wise比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 4])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor((1, 2, -1))\n",
    "b = torch.tensor((3, 0, 4))\n",
    "torch.maximum(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果张量大小不同，也会默认执行广播运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor((1, 2, -1))\n",
    "b = torch.tensor([0])\n",
    "torch.maximum(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理NaN值\n",
    "\n",
    "有时候会有一nan值需要处理。在torch中检测是否有nan值可以使用：x != x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.nan==np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7., nan,  9.],\n",
       "        [10., 11., 12.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5 = torch.FloatTensor([[7,np.nan,9],[10,11,12]])\n",
    "t5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果是直接判断一个多维张量是否有nan值，可以使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True],\n",
       "        [ True,  True,  True]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask= t5==t5\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please check\n"
     ]
    }
   ],
   "source": [
    "if len(mask[mask == True]) > 0:\n",
    "    print(\"please check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.,  9., 10., 11., 12.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t6=t5[mask]\n",
    "t6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让一般的数值计算能够成为梯度可追踪的计算，我们有时候需要将常见的一些计算用tensor重写，下面是NSE的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "def nse_2d(t,p):\n",
    "    seq_length = t.shape[0]\n",
    "    Ngage = t.shape[1]\n",
    "    tmean = torch.mean(t, dim=0)\n",
    "    tmeans = tmean.repeat(seq_length, 1)\n",
    "    SST = torch.sum((t - tmeans) ** 2, dim=0)\n",
    "    SSRes = torch.sum((t - p) ** 2, dim=0)\n",
    "    # Same as Fredrick 2019\n",
    "    # temp = SSRes / ((torch.sqrt(SST) + 0.1) ** 2)\n",
    "    # original NSE\n",
    "    temp = SSRes / SST\n",
    "    loss = torch.sum(temp) / Ngage\n",
    "    return loss\n",
    "\n",
    "t1 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
    "t2 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
    "print(nse_2d(t1,t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对Tensor的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 复制操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似numpy的repeat和tile，torch中可以使用repeat："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 1., 2., 3., 1., 2., 3., 1., 2., 3.],\n",
       "        [4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
    "z.repeat(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [4., 5., 6.],\n",
       "        [4., 5., 6.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.repeat(1,4).view(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.repeat(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3281, -0.8897,  0.7370, -0.9295],\n",
       "         [ 0.7451,  0.3618, -0.4559, -0.7497],\n",
       "         [-0.9598, -1.1446, -2.7180, -1.1157]],\n",
       "\n",
       "        [[ 1.1102, -1.1062, -0.7549,  1.4117],\n",
       "         [ 0.5135, -1.6110,  0.4967,  1.1779],\n",
       "         [ 0.5094,  1.6470,  0.0407,  0.1715]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1102, -1.1062, -0.7549,  1.4117],\n",
       "        [ 0.5135, -1.6110,  0.4967,  1.1779],\n",
       "        [ 0.5094,  1.6470,  0.0407,  0.1715]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[-1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1102, -1.1062, -0.7549,  1.4117],\n",
       "         [ 0.5135, -1.6110,  0.4967,  1.1779],\n",
       "         [ 0.5094,  1.6470,  0.0407,  0.1715]],\n",
       "\n",
       "        [[ 1.1102, -1.1062, -0.7549,  1.4117],\n",
       "         [ 0.5135, -1.6110,  0.4967,  1.1779],\n",
       "         [ 0.5094,  1.6470,  0.0407,  0.1715]],\n",
       "\n",
       "        [[ 1.1102, -1.1062, -0.7549,  1.4117],\n",
       "         [ 0.5135, -1.6110,  0.4967,  1.1779],\n",
       "         [ 0.5094,  1.6470,  0.0407,  0.1715]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[-1,:,:].repeat(3,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 与numpy之间的转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor 转换为numpy："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)    # see how the numpy array changed in value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意tensor变化时，array也会跟着变。反过来也一样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)  # see how changing the np array changed the torch Tensor automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Tensors on the CPU except a CharTensor support converting to NumPy and back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 维度变换\n",
    "\n",
    "再看看维度变换，三维变二维："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.,  8.,  9.],\n",
       "         [10., 11., 12.]],\n",
       "\n",
       "        [[ 7.,  8.,  9.],\n",
       "         [10., 11., 12.]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.FloatTensor([[[7,8,9],[10,11,12]],[[7,8,9],[10,11,12]]])\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.,  8.,  9.],\n",
       "        [10., 11., 12.],\n",
       "        [ 7.,  8.,  9.],\n",
       "        [10., 11., 12.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2=t1.view(-1, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.,  8.,  9.],\n",
       "        [10., 11., 12.],\n",
       "        [ 7.,  8.,  9.],\n",
       "        [10., 11., 12.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3=t1.reshape(-1, 3)\n",
    "t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape 和 view 的区别这里简单补充下，参考了：[Pytorch-reshape与view的区别](https://congluwen.top/2018/12/pytorch_reshape_view/)。Pytorch中reshape()与view()都可以改变Tensor的shape但是有略微的区别.\n",
    "\n",
    "- view()只可以由torch.Tensor.view()来调用，view():\n",
    "    - 不改变Tensor数据，改变Tensor的size(即shape)\n",
    "    - 对于一个将要被view的Tensor，新的size必须与原来的size与stride兼容,即新的维度必须是以下两种情况: \n",
    "        - 是原有维度的一个子空间\n",
    "        - 只跨越原有满足邻接条件，stride[i]=stride[i+1]×size[i+1]的原有维度d,d+1,…,d+k\\\n",
    "    - 否则，在view之前必须调用contiguous()方法，对于该方法的讨论，见[StackOverflow](https://stackoverflow.com/questions/48915810/pytorch-contiguous)\n",
    "- reshape()可以由torch.reshape(),也可由torch.Tensor.reshape()调用。reshape():\n",
    "    - 同样也是返回与input数据量相同，但形状不同的tensor\n",
    "    - 若满足view的条件，则不会copy，若不满足，则会copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0589, -0.2256, -1.2757],\n",
       "        [-0.5401,  1.4478, -1.2537]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0589, -0.5401],\n",
       "        [-0.2256,  1.4478],\n",
       "        [-1.2757, -1.2537]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.t(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
