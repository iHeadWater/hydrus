{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 再探torch.nn\n",
    "\n",
    "pytorch提供了很多设计良好的modules和classed，torch.nn，torch.optim，和Dataset以及Dataloader 来帮助构建和训练神经网络。为了充分利用它们的能力并为自己的问题定制，需要真正地理解它们都在做什么。一个训练神经网络训练识别MNIST数据集的例子。首先只使用最基本的tensor的功能，然后逐步添加torch.nn, torch.optim, Dataset, or DataLoader的特性进去，展示每一块究竟在做什么，以及怎么能让代码更加简洁灵活。\n",
    "\n",
    "## 数据集设置\n",
    "\n",
    "MNIST数据集。用pathlib处理路径。使用requests下载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集是numpy数组格式，已经用pickle序列化存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个图片都是28×28的，展开为一维784 长度存储的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 使用torch.tensor，而不是numpy数组。所以我们需要转换我们的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "n, c = x_train.shape\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不使用torch.nn构建神经网络\n",
    "\n",
    "创建随机tensors，表示权重和偏置，然后告诉这些tensor，它们需要梯度，这就会让PyTorch 记录在tensors上的运算，构建计算图，以使能自动进行反向传播计算。因为初始化这步我们是不想要加入计算图的，所以requires_grad要在初始化之后设置。注意尾部加上_表示变量原地执行该操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为pytorch自动计算梯度的能力，我们可以使用标准的python函数作为model的部分。虽然pytorch准备了很多现成的函数，但到目前为止还是可以轻松地实现自己的版本。pytorch会自动使用GPU加速。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@表示点乘运算，接下来调用函数作用到一个batch的数据上，这称为one forward pass。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.0446, -1.9517, -2.3846, -2.6700, -1.7815, -2.9094, -2.7893, -2.2573,\n",
      "        -2.6186, -2.2513], grad_fn=<SelectBackward>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = x_train[0:bs]  # a mini-batch from x\n",
    "preds = model(xb)  # predictions\n",
    "preds[0], preds.shape\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preds张量不仅包含张量值，也有梯度函数，这会被用来做反向传播。接下来还是直接使用python函数做loss计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在看看目前的loss是多少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2833, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在可以看看模型的精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1094)\n"
     ]
    }
   ],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一次的训练到预测的过程搞定之后，现在可以执行循环，每次迭代的内容是：\n",
    "\n",
    "- select a mini-batch of data (of size bs)\n",
    "- use the model to make predictions\n",
    "- calculate the loss\n",
    "- loss.backward() updates the gradients of the model, in this case, weights and bias.\n",
    "\n",
    "在torch.no_grad()环境下执行更新参数的过程。然后在下次loop之前，将梯度设置为0。否则的话梯度会自动累计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        #         set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经完整地写了一个最小的神经网络了。可以看下现在的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0799, grad_fn=<NegBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 torch.nn.functional\n",
    "\n",
    "现在开始重构代码，使用nn包。每步都使我们的代码更简洁更灵活更易理解。\n",
    "\n",
    "首先就是用torch.nn.functional代替我们手写的激活函数和损失函数。该module下有很多函数可以使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0799, grad_fn=<NllLossBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用nn.Module\n",
    "\n",
    "接下来使用 nn.Module and nn.Parameter，继承nn.Module类，获取权重和偏置并做前向计算。nn.Module有很多属性和方法可以使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2756, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias\n",
    "    \n",
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在利用model.parameters()和model.zero_grad()，使前面的参数更新过程更加简洁，并且更不容易漏掉某些参数。比如：\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()\n",
    "```\n",
    "\n",
    "现在将训练loop放入fit函数，并运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看现在的loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0817, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用nn.Linear\n",
    "\n",
    "使用nn.Linear定义linear层，替代 xb  @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3328, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0814, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)\n",
    "    \n",
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))\n",
    "fit()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用optim\n",
    "\n",
    "torch.optim有很多优化函数。可用它来执行权重更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0804, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(model(xb), yb))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Dataset\n",
    "\n",
    "定义一个子类FacialLandmarkDataset 继承Dataset，重写__len__函数和__getitem__函数。先看看pytorch的TensorDataset，它是一个包含tensors的Dataset。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以直接使用对象的索引得到x和y，而不需要再像下面这样分别索引：\n",
    "\n",
    "```python\n",
    "xb = x_train[start_i:end_i]\n",
    "yb = y_train[start_i:end_i]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0827, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        xb, yb = train_ds[i * bs: i * bs + bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用DataLoader\n",
    "\n",
    "DataLoader 是用来管理batches的，可以从Dataset创建一个DataLoader，它会使数据batches的循环更容易。DataLoader自动地给出每个minibatch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前像这样loop：\n",
    "\n",
    "```python\n",
    "for i in range((n-1)//bs + 1):\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "    pred = model(xb)\n",
    "```\n",
    "\n",
    "现在可以这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0825, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 直接取xb, yb\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 添加验证集\n",
    "\n",
    "为了防止过拟合，验证数据集是必要的。打乱训练数据对防止batches间的相关性和过拟合是很重要的。验证集打不打乱结果是一样的，所以没必要shuffle。\n",
    "\n",
    "验证集的batchsize设置比训练集大，因为验证集不需要反向传播，因此需要更少的memory。所以这里用两倍的训练集batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在在每个epoch最后来计算验证集loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.3152)\n",
      "1 tensor(0.3378)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "\n",
    "    print(epoch, valid_loss / len(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，我们总是在训练之前调用model.train()，这是因为后面会在推断之前调用model.eval()，这时候模型会进入验证模式，所以重新训练之前要先切换回训练模式。它们是用来确保不同的阶段行为是适当的。\n",
    "\n",
    "## 构建fit() 和 get_data()\n",
    "\n",
    "专门写一个loss函数计算每个batch的loss，loss_batch。\n",
    "\n",
    "前向计算的时候需要optimizer，反向的时候是不需要的。\n",
    "\n",
    "然后构建一个fit函数，还有get_data。这样我们就能非常容易地执行我们的代码了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.338123899435997\n",
      "1 0.30423860963582994\n"
     ]
    }
   ],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)\n",
    "        \n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转换为CNN\n",
    "\n",
    "三个卷积层，前面的函数都没有指定模型形式，所以可以直接拿来用以训练 CNN 模型。使用pytorch预定义的Conv2d 类作为卷积层。用三层卷积层定义CNN。每个卷积后是一个ReLU，最后执行一次平均池化，关于CNN的内容可以参考后面 5-cnn-example 文件夹中的文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))\n",
    "\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view 函数就是 pytorch版本的numpy的reshape函数。Momentum  是随机梯度下降的一种变形，它考虑到以前的更新，通常会使得训练更快。因此使用它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4330894303798676\n",
      "1 0.2754692686676979\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_CNN()\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Sequential\n",
    "\n",
    "torch.nn 中还有一个很好用的类，Sequential 。Sequential 对象会以序列的形式运行它之中的每个module。这是一种更简单地编写神经网络的方式。为了利用sequential，对给定的函数，需要继承 nn.Module 定义定制的层。比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后利用该层，就可以构建Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3398027998447418\n",
      "1 0.2788183623671532\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 包装DataLoader\n",
    "\n",
    "现在CNN已经很简洁了，不过它还只能用到MNIST上，因为：\n",
    "\n",
    "- 假设了输入是 28\\*28 向量\n",
    "- 假设了最后的CNN grid size是 4\\*4 （平均池化 kernel size）\n",
    "\n",
    "现在如果取消这两个假设，让模型可以应对任意的 2d 单 channel 图片。首先要除掉初始的Lambda层，将数据处理移到一个generator中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用 nn.AdaptiveAvgPool2d  替换 nn.AvgPool2d ， 这允许我们定义输出tensor的大小为我们想要的而不是我们现在有的。现在就可以这样了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3950530921459198\n",
      "1 0.24604604278206826\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用GPU\n",
    "\n",
    "首先检查自己的GPU是否可用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后可以创建一个设备对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据放入设备对象中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
    "\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型也放入设备中，就可以快速计算了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "在模型训练中，dropout是个比较重要的环节，以下给出一个示例。\n",
    "\n",
    "参考：[Dropout在RNN中的应用综述](https://lonepatient.top/2018/09/24/a-review-of-dropout-as-applied-to-rnns.html)\n",
    "\n",
    "受性别在进化中的作用的启发，Hinton等人最先提出Dropout，即暂时从网络中**移除神经网络中的单元**。 Srivastava等人将Dropout应用于前馈神经网络和受限玻尔兹曼机，其工作的核心概念是“在加入了dropout的神经网络在训练时，每个隐藏单元必须学会随机选择其他单元样本。这理应使每个隐藏的单元更加健壮，并驱使它**自己学到有用的特征，而不依赖于其他隐藏的单元**来纠正它的错误“。\n",
    "\n",
    "在标准神经网络中，每个参数通过梯度下降逐渐优化达到全局最小值。因此，**隐藏单元可能会为了修正其他单元的错误而更新参数**。这可能导致“共适应”，反过来会导致过拟合。而假设通过使其他隐藏单元的存在不可靠，dropout阻止了每个隐藏单元的共适应。\n",
    "\n",
    "随着**数据集变大，dropout的增益增加到一个点然后下降**。 这表明，对于任何给定的网络结构和dropout率，存在一个“最佳点”。\n",
    "\n",
    "用伯努利分别来表示隐藏单元被激活的概率，其中值1为概率p，否则为0。\n",
    "$$P(n)=\n",
    "\\left\\{\\begin{matrix}\n",
    " 1-p  \\ \\ \\  for \\ n=0 \\\\ \n",
    " p    \\ \\ \\ \\ \\ \\ \\ \\  for \\ n=1\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "代码如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "        self.params = []\n",
    "    def forward(self,X):\n",
    "        self.mask = np.random.binomial(1,self.prob,size=X.shape) / self.prob\n",
    "        out = X * self.mask\n",
    "    return out.reshape(X.shape)\n",
    "    def backward(self,dout):\n",
    "        dX = dout * self.mask\n",
    "        return dX,[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Dropout之后，Wan等人的进一步提出了DropConnect，它“通过**随机丢弃权重而不是激活来扩展Dropout**”。 “使用Drop Connect, drop的是每个连接，而不是每个输出单元。”与Dropout一样，该技术仅适用于全连接层。\n",
    "\n",
    "通过将dropout应用于输入权重而不是激活，DropConnect可以推广到全连接网络层的整个连接结构。\n",
    "\n",
    "有dropout的神经网络和没有的区别：\n",
    "\n",
    "一个是在神经网络结构中增加dropout层；另一个，注意dropout有两个mode，一是对train data的，一个是对test data的。在train data中进行dropout，但是在test data中是不用的\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://deeplearningcourses.com/c/data-science-deep-learning-in-theano-tensorflow\n",
    "# https://www.udemy.com/data-science-deep-learning-in-theano-tensorflow\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "# Note: is helpful to look at keras_example.py first\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from util import get_normalized_data\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "# get the data, same as Theano + Tensorflow examples\n",
    "# no need to split now, the fit() function will do it\n",
    "Xtrain, Xtest, Ytrain, Ytest = get_normalized_data()\n",
    "\n",
    "# get shapes\n",
    "_, D = Xtrain.shape\n",
    "K = len(set(Ytrain))\n",
    "\n",
    "# Note: no need to convert Y to indicator matrix\n",
    "\n",
    "\n",
    "# the model will be a sequence of layers\n",
    "model = torch.nn.Sequential()\n",
    "\n",
    "# ANN with layers [784] -> [500] -> [300] -> [10]\n",
    "# NOTE: the \"p\" is p_drop, not p_keep\n",
    "model.add_module(\"dropout1\", torch.nn.Dropout(p=0.2))\n",
    "model.add_module(\"dense1\", torch.nn.Linear(D, 500))\n",
    "model.add_module(\"relu1\", torch.nn.ReLU())\n",
    "model.add_module(\"dropout2\", torch.nn.Dropout(p=0.5))\n",
    "model.add_module(\"dense2\", torch.nn.Linear(500, 300))\n",
    "model.add_module(\"relu2\", torch.nn.ReLU())\n",
    "model.add_module(\"dropout3\", torch.nn.Dropout(p=0.5))\n",
    "model.add_module(\"dense3\", torch.nn.Linear(300, K))\n",
    "# Note: no final softmax!\n",
    "# just like Tensorflow, it's included in cross-entropy function\n",
    "\n",
    "\n",
    "# define a loss function\n",
    "# other loss functions can be found here:\n",
    "# http://pytorch.org/docs/master/nn.html#loss-functions\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "# Note: this returns a function!\n",
    "# e.g. use it like: loss(logits, labels)\n",
    "\n",
    "\n",
    "# define an optimizer\n",
    "# other optimizers can be found here:\n",
    "# http://pytorch.org/docs/master/optim.html\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# define the training procedure\n",
    "# i.e. one step of gradient descent\n",
    "# there are lots of steps\n",
    "# so we encapsulate it in a function\n",
    "# Note: inputs and labels are torch tensors\n",
    "def train(model, loss, optimizer, inputs, labels):\n",
    "    # set the model to training mode\n",
    "    # because dropout has 2 different modes!\n",
    "    model.train()\n",
    "\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    labels = Variable(labels, requires_grad=False)\n",
    "\n",
    "    # Reset gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    logits = model.forward(inputs)\n",
    "    output = loss.forward(logits, labels)\n",
    "\n",
    "    # Backward\n",
    "    output.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # what's the difference between backward() and step()?\n",
    "\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "# similar to train() but not doing the backprop step\n",
    "def get_cost(model, loss, inputs, labels):\n",
    "    # set the model to testing mode\n",
    "    # because dropout has 2 different modes!\n",
    "    model.eval()\n",
    "\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    labels = Variable(labels, requires_grad=False)\n",
    "\n",
    "    # Forward\n",
    "    logits = model.forward(inputs)\n",
    "    output = loss.forward(logits, labels)\n",
    "\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "# define the prediction procedure\n",
    "# also encapsulate these steps\n",
    "# Note: inputs is a torch tensor\n",
    "def predict(model, inputs):\n",
    "    # set the model to testing mode\n",
    "    # because dropout has 2 different modes!\n",
    "    model.eval()\n",
    "\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    logits = model.forward(inputs)\n",
    "    return logits.data.numpy().argmax(axis=1)\n",
    "\n",
    "\n",
    "# return the accuracy\n",
    "# labels is a torch tensor\n",
    "# to get back the internal numpy data\n",
    "# use the instance method .numpy()\n",
    "def score(model, inputs, labels):\n",
    "    predictions = predict(model, inputs)\n",
    "    return np.mean(labels.numpy() == predictions)\n",
    "\n",
    "\n",
    "### prepare for training loop ###\n",
    "\n",
    "# convert the data arrays into torch tensors\n",
    "Xtrain = torch.from_numpy(Xtrain).float()\n",
    "Ytrain = torch.from_numpy(Ytrain).long()\n",
    "Xtest = torch.from_numpy(Xtest).float()\n",
    "Ytest = torch.from_numpy(Ytest).long()\n",
    "\n",
    "# training parameters\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "n_batches = Xtrain.size()[0] // batch_size\n",
    "\n",
    "# things to keep track of\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# main training loop\n",
    "for i in range(epochs):\n",
    "    cost = 0\n",
    "    test_cost = 0\n",
    "    for j in range(n_batches):\n",
    "        Xbatch = Xtrain[j * batch_size:(j + 1) * batch_size]\n",
    "        Ybatch = Ytrain[j * batch_size:(j + 1) * batch_size]\n",
    "        cost += train(model, loss, optimizer, Xbatch, Ybatch)\n",
    "\n",
    "    # we could have also calculated the train cost here\n",
    "    # but I wanted to show you that we could also return it\n",
    "    # from the train function itself\n",
    "    train_acc = score(model, Xtrain, Ytrain)\n",
    "    test_acc = score(model, Xtest, Ytest)\n",
    "    test_cost = get_cost(model, loss, Xtest, Ytest)\n",
    "\n",
    "    print(\"Epoch: %d, cost: %f, acc: %.2f\" % (i, test_cost, test_acc))\n",
    "\n",
    "    # for plotting\n",
    "    train_costs.append(cost / n_batches)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_costs.append(test_cost)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# plot the results\n",
    "plt.plot(train_costs, label='Train cost')\n",
    "plt.plot(test_costs, label='Test cost')\n",
    "plt.title('Cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_accuracies, label='Train accuracy')\n",
    "plt.plot(test_accuracies, label='Test accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "简单复习下步骤：\n",
    "\n",
    "获取数据，包括归一化处理—>构建神经网络结果，主要是定义layer，完成参数初始化—>定义forward函数，主要是每层计算输出，包括定义激活函数—>定义损失函数、优化算法—>注意要先将数据定义到张量中，然后调用函数式地调用神经网络对象以调用forward计算—>计算loss并由loss张量完成自动梯度计算反向传播—>调用优化算法的step完成参数更新—>梯度归零（也可以放在loss反向传播之前），开始新一轮循环参数更新。\n",
    "\n",
    "上述步骤中，除了重写Module之外，直接利用Sequential构建神经网络结构也是一种常见做法。\n",
    "\n",
    "接下来利用pytorch写一个神经网络的示例来自课程《Mordern deep learning in Python》。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in and transforming data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, cost: 0.251691, acc: 0.96\n",
      "Epoch: 1, cost: 0.117666, acc: 0.97\n",
      "Epoch: 2, cost: 0.082470, acc: 0.97\n",
      "Epoch: 3, cost: 0.054635, acc: 0.97\n",
      "Epoch: 4, cost: 0.060924, acc: 0.97\n",
      "Epoch: 5, cost: 0.052717, acc: 0.97\n",
      "Epoch: 6, cost: 0.039973, acc: 0.98\n",
      "Epoch: 7, cost: 0.034754, acc: 0.97\n",
      "Epoch: 8, cost: 0.044111, acc: 0.98\n",
      "Epoch: 9, cost: 0.050160, acc: 0.98\n",
      "Epoch: 10, cost: 0.031708, acc: 0.98\n",
      "Epoch: 11, cost: 0.042624, acc: 0.98\n",
      "Epoch: 12, cost: 0.030264, acc: 0.98\n",
      "Epoch: 13, cost: 0.032563, acc: 0.98\n",
      "Epoch: 14, cost: 0.027656, acc: 0.98\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_normalized_data():\n",
    "    \"\"\"获取数据\"\"\"\n",
    "    print(\"Reading in and transforming data...\")\n",
    "\n",
    "    if not os.path.exists('data/mnist/train.csv'):\n",
    "        print('Looking for data/mnist/train.csv')\n",
    "        print('You have not downloaded the data and/or not placed the files in the correct location.')\n",
    "        print('Please get the data from: https://www.kaggle.com/c/digit-recognizer')\n",
    "        print('Place train.csv in the folder data adjacent to the class folder')\n",
    "        exit()\n",
    "\n",
    "    df = pd.read_csv('data/mnist/train.csv')\n",
    "    data = df.values.astype(np.float32)\n",
    "    np.random.shuffle(data)\n",
    "    X = data[:, 1:]\n",
    "    Y = data[:, 0]\n",
    "\n",
    "    Xtrain = X[:-1000]\n",
    "    Ytrain = Y[:-1000]\n",
    "    Xtest = X[-1000:]\n",
    "    Ytest = Y[-1000:]\n",
    "\n",
    "    # normalize the data\n",
    "    mu = Xtrain.mean(axis=0)\n",
    "    std = Xtrain.std(axis=0)\n",
    "    np.place(std, std == 0, 1)\n",
    "    Xtrain = (Xtrain - mu) / std\n",
    "    Xtest = (Xtest - mu) / std\n",
    "\n",
    "    return Xtrain, Xtest, Ytrain, Ytest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch\n",
    "\n",
    "# 第一步是load data，这里采用的是deep learning里的hello world，即识别手写数字的数据集，来自kaggle。\n",
    "# get the data,\n",
    "# no need to split now, the fit() function will do it\n",
    "Xtrain, Xtest, Ytrain, Ytest = get_normalized_data()\n",
    "\n",
    "# get shapes\n",
    "_, D = Xtrain.shape\n",
    "K = len(set(Ytrain))\n",
    "\n",
    "# 第二步是构建网络结构，使用pytorch构建神经网络要比直接通过numpy手写简单很多，这里先构建一个sequential（和Keras类似），然后逐层添加网络结构即可。\n",
    "model = torch.nn.Sequential()\n",
    "# 逐层添加网络即可，在pytorch中是采用add_module()函数。第一个参数是当前层的命名，可以取任何想要的名称。第二个参数就是该层。层要么是linear transformation，要么是activation\n",
    "# 比如第一层，Linear,参数D是输入的神经元个数，第二个参数500是输出的神经元个数\n",
    "model.add_module(\"dense1\", torch.nn.Linear(D, 500))\n",
    "model.add_module(\"relu1\", torch.nn.ReLU())\n",
    "model.add_module(\"dense2\", torch.nn.Linear(500, 300))\n",
    "model.add_module(\"relu2\", torch.nn.ReLU())\n",
    "model.add_module(\"dense3\", torch.nn.Linear(300, K))\n",
    "# 第三步是构建loss函数，loss函数详情可参考http://pytorch.org/docs/master/nn.html#loss-functions\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "# 第四步是设置优化函数，Adam是一种常用的优化算法，是一种改良的GD算法。算法需要神经网络的parameters作为参数。\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# 第五步是定义训练过程和预测过程，这部分也是相对最难掌握的一部分。这部分相对TensorFlow和Theano都会麻烦一些。\n",
    "\n",
    "\n",
    "def train(model, loss, optimizer, inputs, labels):\n",
    "    \"\"\"训练过程主要包括：包装输入输出到Variable变量，初始化优化函数，前向传播，反向传播，以及参数更新，详情见每步解释\"\"\"\n",
    "    # 为什么要包装变量到Variable：把Tensor包装到Variable中，它就会开始保存所有计算历史。因此每次运算都会稍微多一些cost；另一方面，在训练循环外部对Variable进行计算操作相对容易。不包装也是可以计算的，并且后面的pytorch版本有柯南高就不需要Variable的这一步了\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    labels = Variable(labels, requires_grad=False)\n",
    "\n",
    "    # pytorch的梯度计算是累计的，这对有些神经网络是比较好的，因此这里初始化为0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 直接调用model的前向函数即可得到输出\n",
    "    logits = model.forward(inputs)\n",
    "\n",
    "    # 然后计算loss\n",
    "    output = loss.forward(logits, labels)\n",
    "\n",
    "    # 接着就是进行反向传播计算\n",
    "    output.backward()\n",
    "\n",
    "    # 最后是更新参数\n",
    "    optimizer.step()\n",
    "\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "def predict(model, inputs):\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    logits = model.forward(inputs)\n",
    "    # argmax函数是给出axis维上数组中最大数的索引\n",
    "    return logits.data.numpy().argmax(axis=1)\n",
    "\n",
    "\n",
    "# 第六步定义各类超参数并开始训练过程\n",
    "# 首先是将numpy变量都设置为torch中的张量，注意要指定数据类型\n",
    "Xtrain = torch.from_numpy(Xtrain).float()\n",
    "Ytrain = torch.from_numpy(Ytrain).long()\n",
    "Xtest = torch.from_numpy(Xtest).float()\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 32  # 每个batch的大小\n",
    "n_batches = Xtrain.size()[0] // batch_size  # batch的个数\n",
    "\n",
    "costs = []\n",
    "test_accuracies = []\n",
    "for i in range(epochs):\n",
    "    cost = 0.\n",
    "    for j in range(n_batches):\n",
    "        Xbatch = Xtrain[j * batch_size:(j + 1) * batch_size]\n",
    "        Ybatch = Ytrain[j * batch_size:(j + 1) * batch_size]\n",
    "        cost += train(model, loss, optimizer, Xbatch, Ybatch)\n",
    "\n",
    "    Ypred = predict(model, Xtest)\n",
    "    acc = np.mean(Ytest == Ypred)\n",
    "    print(\"Epoch: %d, cost: %f, acc: %.2f\" % (i, cost / n_batches, acc))\n",
    "\n",
    "    costs.append(cost / n_batches)\n",
    "    test_accuracies.append(acc)\n",
    "\n",
    "# 第七步是将训练过程可视化,# EXERCISE: plot test cost + training accuracy too。一般都是把cost和accuracy都可视化出来\n",
    "# plot the results\n",
    "plt.plot(costs)\n",
    "plt.title('Training cost')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(test_accuracies)\n",
    "plt.title('Test accuracies')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
