{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel Collections\n",
    "---------------------\n",
    "\n",
    "Systems like Spark and Dask include \"big data\" collections with a small set of high-level primitives like `map`, `filter`, `groupby`, and `join`.  With these common patterns we can often handle computations that are more complex than map, but are still structured.\n",
    "\n",
    "In this section we repeat the submit example using the PySpark and the Dask.Bag APIs, which both provide parallel operations on linear collections of arbitrary objects.\n",
    "\n",
    "\n",
    "### Objectives\n",
    "\n",
    "*  Use high-level `pyspark` and `dask.bag` to parallelize common non-map patterns\n",
    "\n",
    "### Requirements\n",
    "\n",
    "*  PySpark\n",
    "\n",
    "*Note: PySpark requires a little additional setup. Usually, the following environment variables need to be set (using `/usr/libexec/java_home` on OS X or similar on Linux)*:\n",
    "\n",
    "```\n",
    "JAVA_HOME\n",
    "JAVA_JRE\n",
    "```\n",
    "\n",
    "*Also, VPNs can interfere with PySpark, so you may need to disable yours if you are running PySpark locally.*\n",
    "\n",
    "windows下的安装使用参考7-parallel-programming/2-distributed-programming.ipynb下的记录。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "We again start with the following sequential code\n",
    "\n",
    "```python\n",
    "series = {}\n",
    "for fn in filenames:   # Simple map over filenames\n",
    "    series[fn] = pd.read_hdf(fn)['x']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for a in filenames:    # Doubly nested loop over the same collection\n",
    "    for b in filenames:  \n",
    "        if a != b:     # Filter out bad elements\n",
    "            results[a, b] = series[a].corr(series[b])  # Apply function\n",
    "\n",
    "((a, b), corr) = max(results.items(), key=lambda kv: kv[1])  # Reduction\n",
    "```\n",
    "\n",
    "### Spark/Dask.bag methods\n",
    "\n",
    "We can construct most of the above computation with the following Spark/Dask.bag methods:\n",
    "\n",
    "*  `collection.map(function)`: apply function to each element in collection\n",
    "*  `collection.cartesian(collection)`: Create new collection with every pair of inputs\n",
    "*  `collection.filter(predicate)`: Keep only elements of colleciton that match the predicate function\n",
    "*  `collection.max()`: Compute maximum element\n",
    "\n",
    "We use these briefly in isolated exercises and then combine them to rewrite the previous computation from the `submit` section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark: Example API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# 所有pyspark的入口都是SparkContext\n",
    "sc = SparkContext('local[4]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(5))  # create collection\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()  # Gather results back to local process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Square each element\n",
    "# pyspark的map函数和python内置的map函数用的方式差不多,虽然它们不一样\n",
    "rdd.map(lambda x: x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Square each element and collect results\n",
    "\n",
    "rdd.map(lambda x: x ** 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only the even elements\n",
    "\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (3, 0),\n",
       " (4, 0),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (3, 2),\n",
       " (4, 2),\n",
       " (3, 3),\n",
       " (3, 4),\n",
       " (4, 3),\n",
       " (4, 4)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cartesian product of each pair of elements in two sequences (or the same sequence in this case)\n",
    "\n",
    "rdd.cartesian(rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (16, 2),\n",
       " (16, 3),\n",
       " (16, 4)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain operations to construct more complex computations\n",
    "\n",
    "(rdd.map(lambda x: x ** 2)\n",
    "    .cartesian(rdd)\n",
    "    .filter(lambda tup: tup[0] % 2 == 0)\n",
    "    .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Parallelize pairwise correlations with PySpark\n",
    "\n",
    "To make this a bit easier we're just going to compute the maximum correlation and not try to keep track of the stocks that yielded this maximal result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\data\\\\json\\\\afl.h5',\n",
       " '..\\\\data\\\\json\\\\bwa.h5',\n",
       " '..\\\\data\\\\json\\\\hal.h5',\n",
       " '..\\\\data\\\\json\\\\hp.h5',\n",
       " '..\\\\data\\\\json\\\\hpq.h5']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "filenames = sorted(glob(os.path.join('..', 'data', 'json', '*.h5')))  # ../data/json/*.json\n",
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Sequential Code\n",
    "\n",
    "series = []\n",
    "for fn in filenames:   # Simple map over filenames\n",
    "    series.append(pd.read_hdf(fn)['close'])\n",
    "\n",
    "results = []\n",
    "\n",
    "for a in series:    # Doubly nested loop over the same collection\n",
    "    for b in series:  \n",
    "        if not (a == b).all():     # Filter out comparisons of the same series \n",
    "            results.append(a.corr(b))  # Apply function\n",
    "\n",
    "result = max(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Parallel code\n",
    "\n",
    "rdd = sc.parallelize(filenames)\n",
    "\n",
    "# TODO\n",
    "\n",
    "result = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%load solutions/collections-1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask.bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%load solutions/collections-2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import dask\n",
    "\n",
    "result = corr.compute(get=dask.local.get_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "*  Higher level collections include functions for common patterns\n",
    "*  Move data to collection, construct lazy computation, trigger at the end\n",
    "*  Used PySpark (`cartesian + map`) and Dask.bag (`product + map`) to handle nested for loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
